\section{研究貢獻與討論}
本研究提出使用模型無關元學習預訓練於資料充足語言，並用以改善資料不足語言的依存句法剖析。
第一章介紹資料不足語言依存句法剖析的問題，並說明其在人工智慧理論與應用上的重要性。
為將語言自身無關句法之特性影響依存句法剖析表現的變因排除，第三章第一部份先使用普適詞性標記做為表徵進行依存句法剖析。
\begin{itemize}
    \item 基準模型在未見過語言上無樣本學習與八十回合上相較其他所提出的方法表現堪稱穩定，
顯示其做為現行資料不足語言依存句法剖析的參數起始點仍具有難以撼動的優勢。
    \item 
\end{itemize}
\section{未來展望}
\subsection{訓練語言的選擇對不同預訓練方法的影響}
本研究預訓練所使用的訓練語言在所有實驗中是固定不變的。
然而訓練語言對預訓練句法剖析模型來說是人類語言所有可能句法空間的取樣樣本，
當面對取樣語言數不足或性質不平衡的取樣語言時，不同模型預訓練演算法不同之處將更容易被突顯出來。
因而探討語言句法空間的取樣偏差（sampling bias）對預訓練模型的影響，也是一項值得深入研究的問題。
\subsection{不同句法樹機率定義對不同預訓練方法的影響}
我們在\ref{subsec:graph_parser}提到過兩種定義句法樹機率的方法，
並只取較簡單的父節點選擇交叉熵法對句法樹機率進行近似。
然其該方法定義的機率終究只是一種近似，對句法樹來說並非合法的機率分佈（所有句法樹的機率總和可能不等於$1$）。
相較於父節點選擇交叉熵法，全域似然性法不在父節點選擇上做正規化，而是對所有句法樹的分數加總計算出其配分函數，
這樣的正規化對句法樹來說方為合法的機率分佈。
雖然全域似然性法在單語言句法剖析時勝過父節點選擇交叉熵法（見\cite{ma-hovy-2017-neural}表3），
我們並不清楚在多語言句法剖析裡，模型必須同時對句法樹機率分佈迥異的多種語言建模的情況下，
全域似然性法相較於父節點選擇交叉熵法是否仍有優勢，
因此我們認為這是未來實驗上值得探討的一個方向。

\subsection{不同依存句法剖析演算法對不同預訓練方法的影響}
本研究的依存句法剖析主要侷限在圖類剖析器的實驗上，
但依存句法剖析尚有轉移類依存句法剖析器（transition-based dependency parser），
將依存句法剖析視為以一個個對樹進行修改的動作將句法樹組裝起來的過程，
有一個緩衝區（buffer）用來貯存尚未處理的詞，一個堆疊區用來表示當下樹的狀態，
以及事先定義好的數個動作（action）將緩衝區的詞一個個接上堆疊區的句法樹，直到完成為止。

阿氏（Wasi Ahmad）曾經在\cite{ahmad-etal-2019-difficulties}中比較過轉移類方法與圖類方法在跨語言轉移學習的優劣，
他發現除了在相近語言（如英文與荷蘭文）間轉移類方法較圖類方法為佳之外，
圖類方法整體來說有較好的跨語言轉移能力，且在不相干的語言間尤甚。
轉移類方法中的不同語言的動作序列（action sequence）差異過大可能是其轉移不易的原因。
然而\cite{ahmad-etal-2019-difficulties}只探討從單一語言（英文）的句法剖析模型出發轉移到其他語言的情況，
主要探討不同單語言句法剖析模型轉移的難易度。
在多語言預訓練的框架下則可探討元學習相較於多語言學習是否可以拉近轉移類方法與圖類方法在跨語言轉移能力的差距。

\subsection{不同編碼器對不同預訓練方法的影響}
阿氏在\cite{ahmad-etal-2019-difficulties}中也比較了轉換器（Transformer）與遞歸類神經網路（RNN）作為依存句法剖析的編碼器在跨語言轉移學習上的表現差異。
他發現轉換器相較於遞歸類神經網路更容易進行跨語言轉移學習。
本研究於去詞化依存句法剖析使用遞歸類神經網路，於詞化依存句法剖析則使用轉換器組成的$\mathrm{mBERT}$，
並沒有在控制其他變因的情況下探討過兩種不同架構的編碼器是否會對元學習與多語言學習在零樣本與精細校正的行為有系統性的影響，
也是一項值得以實驗釐清的問題。
%在多語言預訓練的框架下可以探討元學習相較於多語言學習是否可以拉近轉換器與遞歸類神經網路在跨語言轉移能力的差距。
