\section{簡介}
%文獻上已經提出許多改進資料不足語言的句法剖析的方法，其中多半需要該資料不足語言的特性，用以找尋與其相似的語言進行協同訓練。
%近年來初見於影像領域的模型無關元學習，本來是為了處理少量樣本學習的問題，
%不久後也為資料不足（但資料量大於少量樣本學習）的學習任務所用\cite{gu-etal-2018-meta}，
%不需要事先知道任務的特性，

\iffalse
有名的語言學家杭氏（Noam Chomsky）觀察人類習得語言的過程，
他認為嬰孩學習語言時所接收到的語言輸入是不足以讓他們習得該語言的所有特徵的，
許多特性相異的語法都可以產生這些他們所接收到的語料，
但孩童仍然習得了該語言的正確語法，
因此他推斷人類出生時大腦中即具備有某種語言習得裝置（Language acquisition device），
而語言習得裝置的任務便是從所有與語料匹配的語法中挑選正確的語法
\fi
本章介紹使用模型無關元學習進行多語言預訓練，以幫助資料不足語言的依存句法剖析的系列實驗。

模型無關元學習，本來是為了處理圖片分類領域少量樣本學習的問題，
不久後也為資料不足（但資料量大於少量樣本學習）的學習任務所用，
如古氏（Jiatao Gu）將元學習方法引入資料不足的機器翻譯，
並勝過使用普通多語言學習的基準模型\cite{gu-etal-2018-meta}。
芬氏(Chelsea Finn)在2018年提出的模型無關元學習(model-agnostic meta-learning)
\cite{Finn2017ModelAgnosticMF}為所有使用梯度下降法(gradient descent)進行最佳化的模型的提供了一項簡潔且有效的方法處理資料不足任務。
在語言轉移學習的框架下，其目標是替未見過的語言(unseen languages)尋找一合適參數初始值，使得少量步數梯度更新後，參數在該語言的測試集上表現最佳，
其稱此在少量步數更新就能大幅增進爲見過語言表現的能力為「快速適應」（fast adaptation）。
%其強調使用少量步數進行梯度更新，即是由於資料不足語言資料稀少，過多步數容易過擬合。

在元學習出現以前，
若希望利用相似語言中所蘊含的資訊給予模型語言普遍具備的歸納偏置（下稱普適語言偏置，universal linguistic biases），
多語言學習（multilingual learning）為一主要的方法\cite{caruana1997multitask}。
藉由共享特徵抽取網路並用以同時訓練多種相關語言，
相關語言的資訊得以透過反向傳播（backpropagation）注入類神經網路中，
使模型相較於解釋單一語言，更加偏好能夠解釋所有相關語言的假說，
有效幫助模型達成泛化（generalization）。

然而多語言訓練的目標，是提高訓練語言(training languages)在其測試集(testing set)上的準確率，
而提高訓練語言的準確率，未必就代表在資料不足語言上的準確率也會隨之提高；
有可能出現訓練語言與資料不足語言差異過大，而導致多語言訓練模型無法幫助資料不足語言的任務表現。

不若單純的多語言訓練，模型無關元學習於訓練階段的目標並非提高在訓練語言上的表現，
而是直接最佳化模型在未見過語言上\finetune後的表現，訓練與測試環境沒有不匹配之處，
希望減輕模型只在訓練語言的測試集上有好表現，而無法推廣到資料不足語言上的問題。

本章為了了解模型無關元學習及其各種一階近似的變形（見第\ref{sec:mamls}節）在多語言依存句法剖析上的行為，
去除詞的影響，只使用詞性標記作為特徵進行依存句法剖析。
另外，為了更深入了解不同模型組態（model configurations）對於模型無關元學習系列方法的影響，
也以模型大小為操作變因，試圖了解小模型與大模型是否會改變不同方法對依存句法剖析的表現。
最後，藉由觀察不同方法經過不同精細校正階段後在目標語言方向性分佈的改變軌跡，
試圖分析模型無關元學習是否有達到其宣稱的快速適應的能力。
\iffalse
尤其當目標任務缺乏資料的時候，若使用過於有表現力的假說集合，
易使模型過擬合到目標任務上，
利用相似任務進行多工學習幫助目標任務提升表現的效果尤其顯著。
然而多工學習得到的模型可以在訓練過的所有任務上的測試集有好表現，
但並未保證這樣的好表現可以轉移到相似但未見過的任務上；
而芬氏(Chelsea Finn)提出的模型無關元學習(model-agnostic meta-learning)
\cite{Finn2017ModelAgnosticMF}提供了多工學習之外的另一種方法，
將領域的歸納偏置（inductive bias）注入類神經網路中。
\fi

\section{多語言去詞化依存句法剖析（multilingual delexicalized dependency parsing）}
由於詞化依存句法剖析有太多變因，包括使用的預訓練模型，其對不同語言的偏置等等，都會影響句法剖析模型於目標語言上的行為；
且詞化依存句法剖析參數量較多，使用二次微分的計算量與佔用空間均較大，訓練耗時，
有時甚至會發生用來進行平行矩陣運算的圖形處理器記憶體不足的問題。
因此為了排除語言本身句法以外性質對句法剖析的影響及計算資源的考量，
本節先進行去詞化依存句法剖析的實驗，
也就是只使用句法樹庫提供的詞性標記做為詞的表徵進行句法剖析，
詞化依存句法剖析留待第\ref{chapter:lex_parsing}章再行探討。

\subsection{詞性標記（POS tags）}
UD句法樹庫中大部分的語言均提供兩種詞性標記：專為該語言設計的詞性標記（XPOS），通常為該句法樹庫尚未整合進UD時原本的詞性標記，
與各語言統一的普適詞性標記\cite{petrov-etal-2012-universal}（Universal POS tags, UPOS），
其捨棄各語言細緻的詞性分別，
整合語言間相似性質的詞性，以達到所有語言共享同一組詞性集合的目標。
如前置介系詞（prepositions）與後置介系詞（postpositions）在UPOS的框架下就被整合成介系詞（adpositions）而不做前後置之分。
由於UPOS有更好的跨語言通用性，本研究去詞化依存句法剖析的詞性標記輸入均使用UPOS。

\subsection{修訂版爬蟲類元學習}
原始版本的爬蟲類元學習\cite{nichol2018first}無論是在內循環或外循環的開頭都不會重新啟動內循環的優化器，
當使用有動能（momentum）的優化器如Adam時，當下的梯度更新會受之前用其他語言計算而得的梯度影響，
造成語言間不必要的干擾。
為了避免此現象，原作者將一階動能項$\beta_{1}$設為$0$。
%然而mBERT原始論文中進行精細校正使用的為具有動能項的Adam，
然而初始實驗發現$\beta_{1} = 0$的Adam在精細校正時時會對準確率造成負面影響。
再者，爬蟲類元學習將外循環原始梯度（raw gradient）直接設為各語言內循環（亦即精細校正）前後參數的差異的平均，
此數值大小很大程度上取決於內循環優化器的學習率，
恐與一般模型精細校正時接收到的梯度分佈差異過大。

為處理此問題，本研究稍稍修改了爬蟲類元學習的演算法，
希望在不改變內循環優化器設置的前提下保留爬蟲類元學習加總內循環所有梯度的優點：
與其將內循環梯度設為前後參數的差異的平均 $\phi - U_{\textrm{ADAM}}^{k}(\phi) $，
內循環原始梯度經過內循環優化器處理過後的產物，
本研究將內循環梯度直接設為內循環\textbf{原始}梯度的平均（此處優化器以Adam為例）：
\begin{equation}
    g_{\textrm{REP}} = \mathbb{E}_{\tau}\left[ \frac{1}{k} \sum_{i=1}^{k} \nabla_{\phi_{i}} \mathcal{L}_{\tau} \left( \phi_{i}^{\texttt{adam}} \right) \right]
\end{equation}
其中
\begin{equation}
    \phi_{i}^{\texttt{adam}} = U_{\textrm{ADAM}} \left( \phi_{i - 1}^{\texttt{adam}} \right).
\end{equation}
此數值雖然是由內循環優化器計算出來，但此處取其原始梯度，
受內循環學習率影響較小。
此修訂版爬蟲類元學習與普通的多語言學習的差異比起原始的爬蟲類元學習要來的更小：
外循環的梯度為內循環原始梯度的平均，與多語言學習類似；
但原始梯度仍是由內循環更新過的參數計算出來的（除了內循環的第一步），更接近原本的爬蟲類元學習。
初始實驗發現此修訂版爬蟲類元學習表現不俗，
且相較原始爬蟲類元學習的表現來的更加穩定。
本研究稍稍濫用爬蟲類元學習的名稱，往後皆以爬蟲類元學習指稱本節所提出的修訂版爬蟲類元學習。

\section{實驗設置}
\label{sec:delex_depparse_setting}
\input{tables/delex_parsing/training_languages.tex}
\input{tables/delex_parsing/test_languages.tex}
\input{tables/delex_parsing/hparams.tex}
我們從\conll的53種訓練語言（73個訓練句法樹庫）中選取有官方驗證集（development set）的46種訓練語言（66個訓練句法樹庫）作爲訓練語言；見表\ref{tab:training_languages}。
預訓練完成後，我們分別對該模型進行\zeroshot及\finetune在預訓練中未見過的語言上。
%我們挑選\conll的訓練語言中剩下的只有訓練集而沒有發展集的語言作爲真實資料不足測試語言（true low-resource testing languages，見表\ref{tab:true_lr_testing_languages}）。
%為了觀察控制資料多寡時對不同預訓練方法的影響，
%我們另外挑選了UD 2.5版中8種不在訓練語言中的語言做為模擬資料不足測試語言（simulated low-resource testing languages，見表\ref{tab:sim_lr_testing_languages}）。
我們挑選UD 2.5版中8種不在訓練語言中的語言做為測試用的未見過語言（unseen languages，見表\ref{tab:unseen_languages}）。
實驗在訓練與測試時時均使用正確的斷句、斷詞，並使用句法樹庫提供的正確詞性做為詞的表徵。

至於多語言訓練的部分，孔氏\cite{kondratyuk-straka-2019-75}與烏氏\cite{ustun2020udapter}進行多語言訓練的方法，
是將全部語言的句法樹庫接在一起、在一個小批次(batch)中混合多個句法樹庫訓練。
這樣的做法可能會導致資料量大的語言取樣頻率過高；
我們的方法則是每次更新從全部語言裡取樣$l$種語言，每種語言取樣$b$個句子，一個批次總共有$b \times l$個句子。
不同於孔氏與烏氏，這樣的方法防止模型過度對資料充足語言的特性建模，但也可能使得資料不足語言的句子被過度取樣而產生過擬合的現象。

超參數的設置見表\ref{tab:delex_hparams}。
\section{實驗結果}
以下首先呈現去詞化依存句法剖析的實驗結果，
\subsection{去詞化依存句法剖析}
\input{figs/delex_parsing/linecharts/lang_avgs.tex}
\input{figs/delex_parsing/barplots/bar.tex}
\input{figs/delex_parsing/dir_scatterplots/dir_scatter.tex}
圖\ref{fig:delex_avg}為去詞化依存句法剖析不同預訓練方法產生的模型在目標語言上經過不同步數的\finetune後的測試集LAS數值。
由圖中可以觀察到：
\begin{itemize}
    \item 在未見過語言上，模型無關元學習模型於只精細校正一步的表現及相對於零樣本的進步量均為各方法中之冠，
顯示模型無關元學習的確有快速適應的能力。
    %\item 在未見過語言上，模型無關元學習於
    \item 在訓練語言上，基準模型無論接觸多少目標語言的資料（0回合、1步、1回合、80回合），其表現均大幅贏過模型無關元學習模型，
    這說明基準模型的確達到其訓練目標-同時剖析所有的訓練語言。
    \item 模型無關元學習模型在零樣本學習的表現大幅落後基準模型，說明其優勢主要在只需少量目標語言資料即可快速適應的能力，
    並不適合做為單一多語言模型同時處理多種語言。
    %\item 在訓練語言上在精細校正一步的設置中，模型無關元學習模型在訓練語言上以少量資料精細校正後，於測試集上的表現明顯不如基準模型，但在未見過語言以少量資料精細校正測試集上
\end{itemize}

\subsection{小模型去詞化依存句法剖析}
\label{subsec:result_small_delex_parsing}
\input{figs/delex_parsing/linecharts/lang_avgs_small.tex}
\input{figs/delex_parsing/barplots/bar_small.tex}
\input{figs/delex_parsing/dir_scatterplots/small_dir_scatter.tex}
%\subsection{限制}
%去詞化依存句法剖析使用普適詞性標注在多語言句法剖析中，雖然排除了各個語言句法以外性質對句法剖析的影響


\iffalse
\input{tables/delex_parsing/delex_las_epoch_1.tex}
\input{tables/delex_parsing/delex_las_epoch_1_maml.tex}
\input{tables/delex_parsing/delex_las_epoch_1_reptile.tex}
\input{tables/delex_parsing/delex_las_epoch_1_fomaml.tex}
\fi
