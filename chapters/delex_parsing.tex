\section{簡介}
%文獻上已經提出許多改進資料不足語言的句法剖析的方法，其中多半需要該資料不足語言的特性，用以找尋與其相似的語言進行協同訓練。
%近年來初見於影像領域的模型無關元學習，本來是為了處理少量樣本學習的問題，
%不久後也為資料不足（但資料量大於少量樣本學習）的學習任務所用\cite{gu-etal-2018-meta}，
%不需要事先知道任務的特性，

\iffalse
有名的語言學家杭氏（Noam Chomsky）觀察人類習得語言的過程，
他認為嬰孩學習語言時所接收到的語言輸入是不足以讓他們習得該語言的所有特徵的，
許多特性相異的語法都可以產生這些他們所接收到的語料，
但孩童仍然習得了該語言的正確語法，
因此他推斷人類出生時大腦中即具備有某種語言習得裝置（Language acquisition device），
而語言習得裝置的任務便是從所有與語料匹配的語法中挑選正確的語法
\fi
本章介紹使用模型無關元學習進行多語言預訓練，以幫助資料不足語言的依存句法剖析的系列實驗。

模型無關元學習本來是為了處理圖片分類領域少量樣本學習的問題，
不久後也為資料不足（但資料量大於少量樣本學習）的學習任務所用，
如古氏（Jiatao Gu）將元學習方法引入資料不足的機器翻譯，
並勝過使用普通多語言學習的基準模型\cite{gu-etal-2018-meta}。
芬氏(Chelsea Finn)在2018年提出的模型無關元學習(model-agnostic meta-learning)
\cite{Finn2017ModelAgnosticMF}為所有使用梯度下降法(gradient descent)進行最佳化的模型提供了一項簡潔且有效的方法處理資料不足任務。
在語言轉移學習的框架下，其目標是替未見過的語言(unseen languages)尋找一合適參數初始值，使得少量步數梯度更新後，參數在該語言的測試集上表現最佳，
其稱此在少量步數更新就能大幅增進爲見過語言表現的能力為「快速適應」（fast adaptation）\cite{Finn2017ModelAgnosticMF}。
%其強調使用少量步數進行梯度更新，即是由於資料不足語言資料稀少，過多步數容易過擬合。

在元學習出現以前，
若希望利用相似語言中所蘊含的資訊給予模型語言普遍具備的歸納偏置（下稱普適語言偏置，universal linguistic biases），
多語言學習（multilingual learning）為一主要的方法\cite{caruana1997multitask}。
藉由共享特徵抽取網路並用以同時訓練多種相關語言，
相關語言的資訊得以透過反向傳播（backpropagation）注入類神經網路中，
使模型相較於解釋單一語言，更加偏好能夠解釋所有相關語言的假說，
有效幫助模型達成泛化（generalization）。

然而多語言訓練的目標，是提高訓練語言(training languages)在其測試集(testing set)上的準確率，
而提高訓練語言的準確率，未必就代表在資料不足語言上的準確率也會隨之提高；
有可能出現訓練語言與資料不足語言差異過大，而導致多語言訓練模型無法幫助資料不足語言的任務表現。

不若單純的多語言訓練，模型無關元學習於訓練階段的目標並非提高在訓練語言上的表現，
而是直接最佳化模型在未見過語言上\finetune後的表現，訓練與測試環境沒有不匹配之處，
希望減輕模型只在訓練語言的測試集上有好表現，而無法推廣到資料不足語言上的問題。

本章為了了解模型無關元學習及其各種一階近似的變形（見第\ref{sec:mamls}節）在多語言依存句法剖析上的行為，
去除詞的影響，只使用詞性標記作為特徵進行依存句法剖析。
另外，為了更深入了解不同模型組態（model configurations）對於模型無關元學習系列方法的影響，
以模型大小為操作變因，試圖了解小模型與大模型是否會改變不同方法對依存句法剖析的表現。
其次，調整不同模型無關元學習內循環步數，並觀察其與依存句法剖析的準確率之間的關係。
最後，藉由觀察不同方法經過不同精細校正階段後在目標語言方向性分佈的改變軌跡，
試圖分析模型無關元學習是否有達到其宣稱的快速適應的能力。
\iffalse
尤其當目標任務缺乏資料的時候，若使用過於有表現力的假說集合，
易使模型過擬合到目標任務上，
利用相似任務進行多工學習幫助目標任務提升表現的效果尤其顯著。
然而多工學習得到的模型可以在訓練過的所有任務上的測試集有好表現，
但並未保證這樣的好表現可以轉移到相似但未見過的任務上；
而芬氏(Chelsea Finn)提出的模型無關元學習(model-agnostic meta-learning)
\cite{Finn2017ModelAgnosticMF}提供了多工學習之外的另一種方法，
將領域的歸納偏置（inductive bias）注入類神經網路中。
\fi

\section{多語言去詞化依存句法剖析（multilingual delexicalized dependency parsing）}
由於詞化依存句法剖析有太多變因，包括使用的預訓練模型，其對不同語言的偏置等等，都會影響句法剖析模型於目標語言上的行為；
且詞化依存句法剖析參數量較多，使用二次微分的計算量與佔用空間均較大，訓練耗時，
有時甚至會發生用來進行平行矩陣運算的圖形處理器記憶體不足的問題。
因此為了排除語言本身句法以外性質對句法剖析的影響及計算資源的考量，
本節先進行去詞化依存句法剖析的實驗，
也就是只使用句法樹庫提供的詞性標記做為詞的表徵進行句法剖析，
詞化依存句法剖析留待第\ref{chapter:lex_parsing}章再行探討。

\subsection{詞性標記（POS tags）}
UD句法樹庫中大部分的語言均提供兩種詞性標記：專為該語言設計的詞性標記（XPOS），通常為該句法樹庫尚未整合進UD時原本的詞性標記，
與各語言統一的普適詞性標記\cite{petrov-etal-2012-universal}（Universal POS tags, UPOS），
其捨棄各語言細緻的詞性分別，
整合語言間相似性質的詞性，以達到所有語言共享同一組詞性集合的目標。
如前置介系詞（prepositions）與後置介系詞（postpositions）在UPOS的框架下就被整合成介系詞（adpositions）而不做前後置之分。
由於UPOS有更好的跨語言通用性，本研究去詞化依存句法剖析的詞性標記輸入均使用UPOS。

\subsection{修訂版爬蟲類元學習}
原始版本的爬蟲類元學習\cite{nichol2018first}無論是在內循環或外循環的開頭都不會重新啟動內循環的優化器，
當使用有動能（momentum）的優化器如Adam時，當下的梯度更新會受之前用其他語言計算而得的梯度影響，
造成語言間不必要的干擾。
為了避免此現象，原作者將一階動能項$\beta_{1}$設為$0$。
%然而mBERT原始論文中進行精細校正使用的為具有動能項的Adam，
然而初始實驗發現$\beta_{1} = 0$的Adam在精細校正時時會對準確率造成負面影響。
再者，爬蟲類元學習將外循環原始梯度（raw gradient）直接設為各語言內循環（亦即精細校正）前後參數的差異的平均，
此數值大小很大程度上取決於內循環優化器的學習率，
恐與一般模型精細校正時接收到的梯度分佈差異過大。

為處理此問題，本研究稍稍修改了爬蟲類元學習的演算法，
希望在不改變內循環優化器設置的前提下保留爬蟲類元學習加總內循環所有梯度的優點：
與其將內循環梯度設為前後參數的差異的平均 $\phi - U_{\textrm{ADAM}}^{k}(\phi) $，
內循環原始梯度經過內循環優化器處理過後的產物，
本研究將內循環梯度直接設為內循環\textbf{原始}梯度的平均（此處優化器以Adam為例）：
\begin{equation}
    g_{\textrm{REP}} = \mathbb{E}_{\tau}\left[ \frac{1}{k} \sum_{i=1}^{k} \nabla_{\phi_{i}} \mathcal{L}_{\tau} \left( \phi_{i}^{\texttt{adam}} \right) \right]
\end{equation}
其中
\begin{equation}
    \phi_{i}^{\texttt{adam}} = U_{\textrm{ADAM}} \left( \phi_{i - 1}^{\texttt{adam}} \right).
\end{equation}
此數值雖然是由內循環優化器計算出來，但此處取其原始梯度，
受內循環學習率影響較小。
此修訂版爬蟲類元學習與普通的多語言學習的差異比起原始的爬蟲類元學習要來的更小：
外循環的梯度為內循環原始梯度的平均，與多語言學習類似；
但原始梯度仍是由內循環更新過的參數計算出來的（除了內循環的第一步），更接近原本的爬蟲類元學習。
初始實驗發現此修訂版爬蟲類元學習表現不俗，
且相較原始爬蟲類元學習的表現來的更加穩定。
本研究稍稍濫用爬蟲類元學習的名稱，往後皆以爬蟲類元學習指稱本節所提出的修訂版爬蟲類元學習。

\section{實驗設置}
\label{sec:delex_depparse_setting}
\input{tables/delex_parsing/hparams.tex}
\input{tables/delex_parsing/training_languages.tex}
\input{tables/delex_parsing/test_languages.tex}
我們從\conll的53種訓練語言（73個訓練句法樹庫）中選取有官方驗證集（development set）的46種訓練語言（66個訓練句法樹庫）作爲訓練語言（見表\ref{tab:training_languages}）。
預訓練完成後，我們分別對該模型進行\zeroshot及\finetune在預訓練中未見過的語言上。
%我們挑選\conll的訓練語言中剩下的只有訓練集而沒有發展集的語言作爲真實資料不足測試語言（true low-resource testing languages，見表\ref{tab:true_lr_testing_languages}）。
%為了觀察控制資料多寡時對不同預訓練方法的影響，
%我們另外挑選了UD 2.5版中8種不在訓練語言中的語言做為模擬資料不足測試語言（simulated low-resource testing languages，見表\ref{tab:sim_lr_testing_languages}）。
我們挑選UD 2.5版中8種不在訓練語言中的語言做為測試用的未見過語言（unseen languages，見表\ref{tab:unseen_languages}）。
精細校正時，為排除語料多寡對各語言表現的影響，各語言從中取樣96句句子做為精細校正用語料。
由於批次大小為16（句），因此精細校正1回合相當於梯度更新6步。
為觀察各方法精細校正不同步數後的進步軌跡，每次實驗均呈現各方法精細校正前、精細校正1步（1/6回合）、1回合與80回合的UAS/LAS。
實驗在訓練與測試時時均使用正確的斷句、斷詞，並使用句法樹庫提供的正確詞性做為詞的表徵。


至於多語言訓練的部分，孔氏\cite{kondratyuk-straka-2019-75}與烏氏\cite{ustun2020udapter}進行多語言訓練的方法，
是將全部語言的句法樹庫接在一起、在一個小批次(batch)中混合多個句法樹庫訓練。
這樣的做法可能會導致資料量大的語言取樣頻率過高；
我們的方法則是每次更新從全部語言裡取樣$l$種語言，每種語言取樣$b$個句子，一個批次總共有$b \times l$個句子。
不同於孔氏與烏氏，這樣的方法防止模型過度對資料充足語言的特性建模，但也可能使得資料不足語言的句子被過度取樣而產生過擬合的現象。

超參數的設置見表\ref{tab:delex_hparams}。此處本章以模型大小做為操縱變因，探討其對不同預訓練方法的影響。
其中一般模型的詞性標記嵌入維度、編碼器的輸入維度與編碼器的隱維度均設為100；經過雙向LSTM之後輸出維度為200的向量，
因此將其後的依存邊與依存標籤維度設為200。
而小模型詞性標記嵌入維度、編碼器的輸入維度與編碼器的隱維度則設為10，經過雙向LSTM之後輸出維度為20的向量，
其後的依存邊與依存標籤維度設為20，均為一般模型的十分之一。

\section{實驗結果}
以下比較去詞化依存句法剖析各方法表現、分析不同內循環步數對各種模型無關元學習方法的影響、
並觀察小模型在上述設置下與普通模型有何不同之處。
\subsection{去詞化依存句法剖析不同方法比較}
\input{figs/delex_parsing/linecharts/lang_avgs.tex}
圖\ref{fig:delex_avg}為去詞化依存句法剖析不同預訓練方法產生的模型在目標語言上經過不同步數的\finetune 後的測試集LAS數值。
由圖中可以觀察到：
\begin{itemize}
    \item 在訓練語言上，基準模型無論接觸多少目標語言的資料（0回合、1步、1回合、80回合），其表現均大幅贏過模型無關元學習模型，
這說明基準模型的確達到其訓練目標-同時剖析所有的訓練語言。
    \item 在未見過語言上，模型無關元學習模型精細校正1步的LAS相對於精細校正前LAS的進步量贏過基準模型與爬蟲類元學習模型，
顯示模型無關元學習方法的確有快速適應的能力，但其代價是在精細校正前的LAS大幅落後另外兩者；
且精細校正1步、1回合與80回合的表現均略遜基準模型與爬蟲類元學習模型，
說明快速適應後的模型無關元學習無法利用更多的資料與更多的精細校正步數持續精進。
    \item 在未見過語言上，爬蟲類元學習模型精細校正前、精細校正1步與1回合的表現均可與基準模型匹敵或略勝一籌，
    只有在精細校正80回合的準確率稍遜基準模型，顯示其做為基準模型與模型無關元學習混和的方法，
既取基準模型同時剖析所有語言的能力，
又能如模型無關元學習模型在接觸少量未見過語言句法資料下有所進步，為模型無關元學習方法中最佳。
    \item 在所有語言上，一階模型無關元學習模型精細校正前後的分數雖然落後其他所有方法，
但在精細校正一步後展現出所有模型最大的進步量（相對於精細校正前）；
顯示其預訓練後的參數中仍蘊含快速適應所需的資訊，
惟精細校正前大幅落後的數值說明其做為同時剖析所有語言的模型可能較不適合。
%這可能是由於其只取內循環最後一步的梯度更新的緣故，浪費了前面所有步數的梯度資訊，使得其與爬蟲類元學習模型有懸殊差別。
\end{itemize}
每個語言各自詳細的UAS/LAS數值可見圖\ref{fig:bar_zs}、\ref{fig:bar_one_step}、
\ref{fig:bar_full_epoch_1}與\ref{fig:bar_full_epoch_80}。
\subsection{去詞化依存句法剖析各方法不同內循環步數比較}
\input{figs/delex_parsing/linecharts/lang_avg_steps.tex}
圖\ref{fig:delex_avg_maml}、\ref{fig:delex_avg_reptile}與\ref{fig:delex_avg_fomaml}
為去詞化依存句法剖析各個預訓練方法不同內循環步數在目標語言上經過不同步數的\finetune 後的測試集LAS數值。
未註明內循環步數之方法其步數皆為2步。
我們有幾項發現：
\begin{itemize}
    \item 圖\ref{fig:delex_avg_maml}中，模型無關元學習-2步在精細校正前與精細校正1步均贏過模型無關元學習-4步，
    而模型無關元學習-4步則在精細校正1回合與80回合略勝一籌，
    說明內循環步數愈長、達到好表現所需要的精細校正步數愈長；
    短內循環步數更快適應未見過語言，但長內循環步數經過較多的精細校正步數後效果愈佳，
    均與其各自優化的目標相符。
    \item 圖\ref{fig:delex_avg_reptile}中，爬蟲類元學習-2步在訓練語言上稍較爬蟲類元學習-4步為佳，
    但在測試語言上爬蟲類元學習-4步卻略為反超爬蟲類元學習-2步，
    可看出爬蟲類元學習愈長的內循環步數可能愈不容易過擬合在訓練語言上，而能夠推廣至未見過的語言上。
    \item 圖\ref{fig:delex_avg_fomaml}中，
一階模型無關元學習-4步在精細校正前表現大幅落後一階模型無關元學習-4步，甚至無預訓練的單語言精細校正80回合都贏過它，
但卻在接觸目標語言一回合（6步）後急起直追追上一階模型無關元學習-2步，快速適應的能力是所有預訓練方法中最好，
顯示其預訓練後（精細校正前）的參數雖然表現差勁，其參數中可能其實蘊含快速適應所需的資訊。
雖然所有精細校正階段的數值仍皆略遜基準模型，其快速適應的能力著實值得更多實驗探究背後成因。
\end{itemize}
\subsection{去詞化依存句法剖析小結}
\begin{itemize}
    \item 我們發現爬蟲類元學習在訓練語言與未見過語言上，無論是精細校正前或經過不同步數的精細校正，其表現均穩定的與基準模型匹敵或稍稍勝過，
顯示其無論做為同時剖析所有語言的單一模型，或者作為資料不足語言剖析器的初始參數於其上進行精細校正都相當合適。
    \item 模型無關元學習在未見過語言上進行小步數的精細校正較其他方法略有優勢，
說明其較基準方法更適合做為資料不足語言剖析器的初始參數，在目標語言語料極少或只容許少量梯度更新的情境下使用可以收到不錯的效果。
    \item 一階模型無關元學習雖然在數值上劣於其他預訓練方法，但其精細校正後與未接觸目標語言前相比的進步量為所有方法之冠，
似乎較模型無關元學習更有快速適應的能力。
\end{itemize}
\subsection{小模型去詞化依存句法剖析不同方法比較}
\label{subsec:result_small_delex_parsing}
\input{figs/delex_parsing/linecharts/lang_avgs_small.tex}
圖\ref{fig:delex_avg}為小模型的去詞化依存句法剖析不同預訓練方法產生的模型在目標語言上經過不同步數的\finetune 後的測試集LAS數值。
我們可以發現幾點與普通大小模型不同的現象：
\begin{itemize}
    \item 預訓練過的模型其LAS數值從60左右降到35-45的區間，有約15-25的降幅。在同樣梯度步數更新的情況下，
    模型變小導致表現變差是正常的現象。
    \item 方法的表現依照與基準模型演算法相似程度排序，愈類似基準模型的方法表現愈好：
與基準方法最像的爬蟲類元學習居次；同為一階但只取內循環最後一步梯度的一階模型無關元學習在後；
而使用二階微分的模型無關元學習則殿底。
且方法之間的表現排序在所有語言與各種精細校正步數下均一致，不似普通大小模型各有千秋。
\end{itemize}
每個語言各自詳細的UAS/LAS數值可見圖\ref{fig:bar_small_zs}、\ref{fig:bar_small_one_step}、
\ref{fig:bar_small_full_epoch_1}與\ref{fig:bar_small_full_epoch_80}。
\subsection{小模型去詞化依存句法剖析各方法不同內循環步數比較}
\input{figs/delex_parsing/linecharts/lang_avg_steps_small.tex}
圖\ref{fig:delex_small_avg_maml}、\ref{fig:delex_small_avg_reptile}與\ref{fig:delex_small_avg_fomaml}
為去詞化依存句法剖析各個預訓練方法不同內循環步數在目標語言上經過不同步數的\finetune 後的測試集LAS數值。
未註明內循環步數之方法其步數皆為2步。

從這些圖中，我們發現愈多內循環步數的模型其表現愈差。
若將愈多內循環步數詮釋為與基準方法愈不像，
那麼我們其實得到與第\ref{subsec:result_small_delex_parsing}節類似的觀察：
在小模型的去詞化依存句法剖析中，與基準方法愈類似的方法，其表現愈好。

\subsection{小模型去詞化依存句法剖析小結}
與普通模型不同，在小模型去詞化依存句法剖析中我們發現簡單的基準模型的表現勝過其他所有模型無關元學習的系列方法；
且與基準模型演算法愈為相似的方法表現愈好。
我們需要更多的實驗與分析來幫助解釋這樣的現象。

這裡我們提出一個可能的原因：或許大模型的參數夠多，使得為所有語言共同建模的參數比例變少，
為各別語言建模的參數比例變多，造成其預訓練出來的模型不易推廣到未見過的語言。
此時模型無關元學習可以促使大模型提高為所有語言共同建模的參數比例，使其更偏好能夠解釋所有語言的假說，
而非只是個別語言句法剖析模型的聯集而已。
然而在小模型的情境下，基準模型沒有足夠的參數為個別語言建模，又需要利用有限的參數在訓練語言上取得好表現，
因此偏好選擇能夠解釋更多語言的假說，以提高參數利用的效率。因此小模型的基準模型比較沒有死記硬背各語言的問題，
在這種情況下模型無關元學習相較於基準模型就沒有特別的優勢了。

\section{分析與討論}
在此章節中，我們想要探討各種預訓練方法在精細校正的過程對目標語言特性的適應方式有何不同：
是從接觸目標語言前就充分掌握目標語言特性，抑或是在接觸的過程中快速適應？

我們挑選語言形態學中常被提到的中心語方向性參數（head-directionality parameter）作為觀察的對象，
在第\ref{subsec:head-dir}節時介紹過，該特性為各語言句法主要的差異之一，
因此各方法產生的句法樹之中心語方向性分佈可作為其對該語言語法的掌握程度重要的指標。
\subsection{計數模型}
有些中心詞方向性特別強烈的語言，我們並不清楚其詞性標記序列分佈是否有可能使模型容易產生中心詞方向性強烈的句法樹。
如日語是非常遵守中心詞後置原則的語言，
經常作為中心詞（如動詞）的詞性可能本來就會置於句子後半，我們並不清楚這樣的現象與剖析出中心詞後置的依存關係相關性高不高。
為了排除語言（及其句法樹庫）的詞性標記序列本身可能造成之對方向性的偏置，
我們需要製作一個不仰賴上下文、
且只含有適用於所有語言的句法規則，而不偏重任一語言特殊規則的剖析器模型來剖析各語言句法樹庫的詞性標記序列。
這樣的模型可以做為判斷預訓練方法有沒有習得目標語言中心語方向性的虛無假設（null hypothesis）。
倘若預訓練方法產生的句法樹其中心語方向性與正確句法樹中心語方向性的相關性不高於上述模型對應的的相關性，
則我們不能宣稱該預訓練方法學到了目標語言的中心語方向性。
% 那麼詞性標記序列的分佈可能真的會造成對方向性的偏置；
% 反之，若模型剖析出來的句法樹方向性與正確句法樹方向性相關性低，

因此，我們訓練了一個使用所有訓練語言語料統計出的計數模型，簡介如下：
給定中心詞詞性$\textrm{p}_{\textrm{head}}$、依附詞詞性$\textrm{p}_{\textrm{dep}}$
及他們的位置$i_{\textrm{head}}$、$i_{\textrm{dep}}$
統計所有訓練語言句法樹庫的中心詞選擇條件機率
$p (\textrm{p}_{\textrm{head}}|\textrm{p}_{\textrm{dep}}, (i_{\textrm{head}}$-$i_{\textrm{dep}}))$，
其中條件為依附詞詞性及中心詞、依附詞兩者的相對位置$i_{\textrm{head}}$-$i_{\textrm{dep}}$。
%進行剖析時，給定一詞性標記序列$(\textrm{p}_{1}, \textrm{p}_{2}, ..., \textrm{p}_{N})$，
%$p(\textrm{p}_{j} | \textrm{p}_{i}) = \frac{p(\textrm{p}_{head=j}|\textrm{p}_{\textrm{dep}=i}, i_{\textrm{head}}$-$i_{\textrm{dep}}}{}$
為了平衡各語言句法樹庫大小不一的情形，上述機率會先在各自語言內部進行正規化，之後再以語言為單位進行正規化，
以達到類似第\ref{sec:delex_depparse_setting}節中平衡各語言句法樹庫取樣頻率的效果。
\subsection{去詞化依存句法剖析各方法產生句法樹之方向性分析}
圖\ref{fig:dir_scatter_zs}、\ref{fig:dir_scatter_one_step}、\ref{fig:dir_scatter_full_epoch_1}與\ref{fig:dir_scatter_full_epoch_80}
呈現了去詞化依存句法剖析各預訓練方法經過不同步數的精細校正後產生的句法樹其方向性的變化及與正確句法樹方向性的差異。

由圖\ref{fig:dir_scatter_zs}可以看出在精細校正前，爬蟲類元學習與基準模型預測出的中心詞方向性就與正確答案高度相關，
顯示這兩種方法無論有沒有接觸過目標語言，在不知道目標語言任何資訊的情況下，透過詞性標記序列就可以充分掌握目標語言之中心詞方向性。
\input{figs/delex_parsing/dir_scatterplots/dir_scatter.tex}
\subsection{小模型去詞化依存句法剖析各方法產生句法樹之方向性分析}
\input{figs/delex_parsing/dir_scatterplots/small_dir_scatter.tex}
%\subsection{限制}
%去詞化依存句法剖析使用普適詞性標注在多語言句法剖析中，雖然排除了各個語言句法以外性質對句法剖析的影響


\iffalse
\input{tables/delex_parsing/delex_las_epoch_1.tex}
\input{tables/delex_parsing/delex_las_epoch_1_maml.tex}
\input{tables/delex_parsing/delex_las_epoch_1_reptile.tex}
\input{tables/delex_parsing/delex_las_epoch_1_fomaml.tex}
\fi
